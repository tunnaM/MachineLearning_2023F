---
title: 'Lab: 树集成'
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.11.2
  kernelspec:
    display_name: R
    language: R
    name: ir
---
# Bagging and Random Forests



我们对`Boston`数据运用bagging和随机森林方法，可通过R包`randomForest`实现。本章的具体运行结果取决于你电脑所安装的`R`的版本和`randomForest`的版本。因为bagging是随机森林在$k=d$下的特例，因此`randomForest()`函数能被用于实现随机森林和bagging两种方法，我们依照下面的代码实施bagging算法：

```{r,message=FALSE}
library(ISLR2)
library(randomForest)
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
bag.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 12, importance = TRUE)
bag.boston
```

参数`mtry = 12`意味着对于树的每一个节点都要考虑全部$12$个变量，换言之，我们执行了bagging。那么这个模型在测试集上的表现如何？

```{r}
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
boston.test <- Boston[-train, "medv"]
plot(yhat.bag, boston.test)
abline(0, 1)
mean((yhat.bag - boston.test)^2)
```

利用bagged回归树，测试集的MSE为$23.40$，约为最优剪枝单一树的三分之二。我们可以通过修改`randomForest()`中的`ntree`参数来修改生成树的数量：

```{r}
bag.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 12, ntree = 25)
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
mean((yhat.bag - boston.test)^2)
```



接下来，我们按照相同的方式建造随机森林，有一点不同的是，我们将使用一个更小的`mtry`值。默认情况下，在建造随机森林回归树的时候，`randomForest()` 会使用 $p/3$个随机变量，建造随机森林分类树的时候会使用$\sqrt{p}$个随机变量。这里我们采用`mtry = 6`。

```{r}
rf.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[-train, ])
mean((yhat.rf - boston.test)^2)
```


测试集的MSE为$20.07$，这意味着在这个例子上随机森林的效果要明显好于bagging方法。
通过`importance()`函数，我们可以看到每个变量的重要性。

```{r}
importance(rf.boston)
```


这里展示了两种变量重要性的度量方式，第一个是基于一个给定的特征被置换的情况下OOB样本预测准确性的平均下降程度，第二个是拆分特定特征的情况下节点非纯度平均减少量，这里要求对所有的树取平均。在回归树的情况下，节点非纯度通过RSS来度量，而对于分类树则通过信息熵来度量。可以使用`varImpPlot()`函数生成这些重要性度量的图。

```{r}
varImpPlot(rf.boston)
```

结果表明，在随机森林中考虑的所有树中，社区的财富（`lstat`）和房屋大小（`rm`）是最重要的两个变量。



# GBDT

这里，我们使用`gbm`包，以及其中的`gbm()`函数。`gbm()`函数默认调用Friedman提出的GBDT方法，并对其做了一定的调整，比如固定的学习率。

因为这是一个回归问题，我们将`gbm()`的参数设定为`distribution = "gaussian"`；如果这是一个二进制分类问题，我们应使用`distribution = "bernoulli"`。也可以设定参数`n.trees = 5000`表示我们想生成$5000$个树，`interaction.depth = 4` 限制了每个树的深度。

```{r,message=FALSE}
library(gbm)
set.seed(1)
boost.boston <- gbm(medv ~ ., data = Boston[train, ], distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
boost.boston
```




我们现在来预测测试集上的`medv`：

```{r}
yhat.boost <- predict(boost.boston, newdata = Boston[-train, ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
```


测试集的MSE是$18.39$：这优于随机森林和bagging的测试集上的MSE。如果需要，我们可以使用一个不同的学习率$\lambda$来实施boosting。默认值是$0.001$，不过很容易修改。这里我们选择$\lambda=0.2$。

```{r}
boost.boston <- gbm(medv ~ ., data = Boston[train, ], distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = 0.2)
yhat.boost <- predict(boost.boston, newdata = Boston[-train, ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
```

在这种情况下，$\lambda=0.2$ 下测试集的MSE 会低于 $\lambda=0.001$ 情况下的MSE.


# XGBoost
```{r,collapse=TRUE}
library(xgboost)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')

train <- agaricus.train
names(train)
dim(train$data)

test <- agaricus.test
dim(test$data)

xgb <- xgboost(data = train$data, label = train$label, max_depth = 2, eta = 1,
               nrounds = 10, objective = "binary:logistic")
pred <- predict(xgb, test$data)
pred[1:10]
pred.label=1*(pred>0.5)+0*(pred<0.5)
table(test$label,pred.label)
```


# 练习 & 第四次作业

1. This problem involves the `OJ` data set which is part of the `ISLR2` package.

(a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.
(b) Fit a tree to the training data, with `Purchase` as the response and the other variables as predictors. Use the `summary()` function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have?
(c) Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed.
(d) Create a plot of the tree, and interpret the results.
(e) Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?
(f) Apply the `cv.tree()` function to the training set in order to determine the optimal tree size.
(g) Produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.
(h) Which tree size corresponds to the lowest cross-validated classification error rate?
(i) Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.
(j) Compare the training error rates between the pruned and unpruned trees. Which is higher?
(k) Compare the test error rates between the pruned and unpruned trees. Which is higher?


2. We now use boosting to predict `Salary` in the `Hitters` data set.

(a) Remove the observations for whom the salary information is unknown, and then log-transform the salaries.
(b) Create a training set consisting of the first 200 observations, and a test set consisting of the remaining observations.
(c) Perform boosting on the training set with 1,000 trees for a range of values of the shrinkage parameter λ. Produce a plot with different shrinkage values on the x-axis and the corresponding training set MSE on the y-axis.
(d) Produce a plot with different shrinkage values on the x-axis and the corresponding test set MSE on the y-axis.
(e) Compare the test MSE of boosting to the test MSE that results from applying linear regression and LASSO.
(f) Which variables appear to be the most important predictors in the boosted model?
(g) Now apply bagging to the training set. What is the test set MSE for this approach?


# 第五次作业
课程已经接近尾声，谈谈你对于机器学习这门课程的收获与感悟，以及对课程建设的一些看法与建议。


**要求**

- 12月18日周日晚24点截止上交，上交pdf文件（一定要pdf，否则无法批改，可以Knit直接生成或html转存）至邮箱：lyfsufe@163.com
- 务必创建一个新的Rmd文件，不要使用我们的教学文档直接上交作业






